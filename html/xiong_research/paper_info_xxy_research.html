<!doctype html>
<html>
<style>
	.container{
		width: 1200px;
		height: 800px;
		margin: auto;
		text-align: center;	
	
				
	}
	.paper_title{
		font-size: 22px;
		color: green;
		width: 1000px;
		float: left;
		position: relative;
		top: 200px;
	}
	.paper_image{
		float: left;
		position: relative;
		left: 18px;
		top: 250px;	
	}
	.paper_time{
		color: gainsboro;
		width: 300px;
		float: left;
		font-size: 14px;
		position: absolute;
		top: 500px
		
		
		
	}
	.paper_info{
		width: 900px;
		text-align: left;
		position: relative;
		top: 250px;
		left: 70px;
		float:left;
		line-height: 30px;
		font-size: 15px
	}
 .bottom_info {
    width: 100%;
    height: 150px;
    background-color: white;
	float: left;
	margin-top: 500px;
  }
	#group_info{
		font-size: 15px;
		margin-top: 30px;
		float: left;
		margin-left: 200px;
		
	}
#group_detail_info{
		clear: both;
		font-size: 15px;
		margin-top: 30px;
		float: left;
		margin-left: 200px;

	}
	.image_logo{

		margin-top: 2px;
	}
	.return_index{
		color:green;
		float: left;
		position: relative;
		top: 270px;
		left: 370px;
		
	}
	.return_index_a{
		text-decoration: none;
		color: green;
	}
	.return_index_a:hover{
		text-decoration: underline;
		color: darkgreen;
	}
	.project_leaders{
		 color:green;
		font-size: 22px;
		text-align:left;
		width: 1200px;
		float: left;
		position: relative;
		top: 300px;
		
	
		
	}
	.project_leaders_name{
		color:black;
		font-size: 16px;
		text-align:left;
		width: 1200px;
		float: left;
		position: relative;
		top: 320px;
		
	
	}
	.project_example{
		 color:green;
		font-size: 22px;
		text-align:left;
		width: 1200px;
		float: left;
		position: relative;
		top: 500px;
	}
	.project_example_images{
		width: 1200px;
		float: left;
		position: relative;
		top: 500px;
		text-align: center;
		margin-top: 20px;
		
	}
	.project_example_images_{
		float: left;
		margin:15px;
		text-align: center
	}
	.example_text{
		width:1200px;
		text-align: center;
		font-size: 15px;
		line-height: 30px;
		float: left;
		
	}
</style>
<head>
<meta charset="utf-8">
<title>paper_info</title>
</head>

<body>
		<div class="container">
				<div class="paper_title">Breast cancer detection and diagnosis</div>
				<div class="paper_image"><img src="xiong_image.jpg"  width="300px" height="197px"></div>
				<div class="paper_time">Published Wed 06 September 2023</div>
				<div class="paper_info">The application of computer-aided technology in medical imaging has made significant advancements, particularly in the field of breast cancer detection and diagnosis. Our research focuses on developing computer-aided systems for breast cancer detection and diagnosis using various medical imaging techniques such as X-ray, CT, MRI, and ultrasound.<p></p>

Breast tumor recognition and classification have become a prominent area of research in computer-aided diagnosis (CAD). The key components of breast image classification include image pre-processing, feature extraction, feature selection, discrimination between benign and malignant tumors, and disease prediction modules. Feature extraction plays a crucial role in this process. Traditional machine learning methods that rely on hand-crafted features often lack robustness in accurately classifying benign and malignant breast tumors. In recent years, deep convolutional neural network (CNN) models have been proposed for object detection and classification tasks. CNN-based methods have shown great promise in extracting high-dimensional abstract features from ultrasound images, leading to high-performance analysis in breast cancer diagnosis. However, deep learning models typically require training on large datasets for improved generalization.<p></p>

Collecting large datasets of medical images is challenging, particularly due to issues related to patient privacy. Additionally, the process of labeling medical ultrasound images adds to the workload of physicians. Our mission is clear: to enhance the accuracy of benign and malignant breast tumor detection and diagnosis using existing data by leveraging transformer and generative AI synthesized images.<p></p>

By utilizing transformer models and generative AI techniques, we aim to improve the performance of breast cancer detection and diagnosis with limited available data. Transformer models have shown remarkable capabilities in capturing complex dependencies in data, while generative AI can generate synthetic images that augment the existing dataset. These synthesized images can help in training deep learning models to improve their generalization and performance.<p></p>

Our research is driven by the goal of enhancing the accuracy of breast tumor detection and diagnosis, even when faced with limited data availability. By leveraging the power of transformer models and generative AI, we aim to develop robust computer-aided systems that can assist medical professionals in accurately identifying and classifying benign and malignant breast tumors.<p></p>
 
</div>
				<div class="return_index"><a  class="return_index_a" href="../research_info.html" >← Back to overview</a></div>
				<div class="project_leaders">Project leaders</div>
				<div class="project_leaders_name">XiangyuXiong</div>
				<div class="project_leaders" style="margin-top:50px">Partner Organisations</div>
				<div class="project_leaders_name">杭州市第一人民医院 </div>
				<div class="project_example">Project Example</div>
				<div class="project_example_images">
						<div class="project_example_images_"><img src="example_1.png" height="200px" width="200px" ></div>
						<div class="project_example_images_" ><img src="example_2.png" height="200px" width="200px" ></div>
						<div class="project_example_images_"><img src="example_3.png" height="200px" width="200px" ></div>
						<div class="project_example_images_" ><img src="example_4.png" height="200px" width="200px" ></div>
					<div class="example_text">These pictures show the detection results of our breast cancer detection model in the ABUS data set, and the masses in the red border box are the breast masses detected by the model.</div>
					
			</div>
			
			
			<div style="float:left;padding-top:400px">
			
					<div class="paper_title">Automatic Detection of Breast Lesions in Automated 3D Breast Ultrasound with Cross-Organ Transfer Learning</div>
				<div class="paper_image"><img src="zhengrui/picture1.png"  width="300px" height="197px"></div>
				<div class="paper_time">Published Tue 21 May 2024</div>
				<div class="paper_info">Breast cancer is a widespread disease that endangers the declaration, effective detection and diagnosis methods are needed to improve the prognosis of patients. At the present stage, most breast cancer screening relies on 2D breast ultrasound, which requires radiologists to imagine the 3D composition of breast lesions. Automated 3D Breast Ultrasound (ABUS) solves this problem very well, and more and more computer-assisted diagnosis (CAD) software developed for ABUS is emerging to reduce the burden on radiologists. However, due to the relatively small popularity of ABUS and the protection of patient privacy by laws and regulations, training CAD software often encounters the problem of small datasets, which limits the software’s performance.<p></p>

We proposed a cross-organ and cross-modality transfer learning method. The detection experience of lung nodules was transferred to the detection of breast lesions and compared with the results transferred from the breast MRI and ABUS datasets. And we also proposed two contrastive learning methods based on BI-RADS grading, which perform contrastive learning at both lesion-level and grade-level. We explored the possibility of using clinical indicators instead of gold standards in deep learning.<p></p>

The experimental results on lung nodules, breast Magnetic Resonance Imaging (MRI), and ABUS datasets show that the detection experience of transferring lung nodules in breast cancer detection tasks effectively improves the model’s performance, with average sensitivity increased by up to 4.82%. In the ABUS dataset with a relatively large amount of data, transferring lung nodule detection experience achieved better results compared to transferring breast MRI detection experience, performance improvement increased by 1.03%. The experimental results on the ABUS dataset showed that both methods helped improve model performance, and the contrastive learning method at the lesion-level showed better performance, with a maximum sensitivity improvement of 3.38%.<p></p>

 
</div>
				<div class="return_index"><a  class="return_index_a" href="../research_info.html" >← Back to overview</a></div>
				<div class="project_leaders">Project leaders</div>
				<div class="project_leaders_name">Zhengrui Huang</div>
				<div class="project_leaders" style="margin-top:50px">Partner Organisations</div>
				<div class="project_leaders_name">杭州市第一人民医院 </div>
				<div class="project_example">Project Example</div>
				<div class="project_example_images">
						<div class="project_example_images_"><img src="zhengrui/picture1.png"  width="800px" ></div>
						<div class="project_example_images_" ><img src="zhengrui/picture2.png" width="800px" ></div>
				
						<div class="example_text">Predict case from our model, the red box represents the possible lesions predicted by our model.</div>
					
			</div>
			</div>
			
			
		</div>
	

	<div style="padding-bottom: 50px;height: 100px;width: 1200px;float: left"></div>
			
				
	
	 <div class="bottom_info"> 
				  <div  style="width:1200px;height: 150px;margin: auto">
				  		<div id="group_info"> © Generalized Electric Medicine 2023 of Macao polytechnic university.</div>
						<div id="group_detail_info">The Generalized Electric Medicine is part of the Macao polytechnic university.</div>
						<div class="image_logo"> <img src="../gem_logopng.png"  style="height:140px;right: 0" ></div>
			       </div>
			  </div>
</body>
</html>
